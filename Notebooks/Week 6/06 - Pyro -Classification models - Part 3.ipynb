{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Classification models  \n",
    "\n",
    "## Part 3: Travel mode choice - Probit regression\n",
    "\n",
    "In this part we will revisit our real world problem of travel model choice. \n",
    "\n",
    "The first part is very similar to previous notebook for part 2: loading data, preprocessing, train/test split, etc. However, in this part, we will consider a Probit regression model. For the sake of simplicty, lets assume that we are just interested in distinguishing between car vs non-car (binary classification problem).\n",
    "\n",
    "Lets just start running the parts corresponding to imports, data loading, preprocessing, train/test split, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import pystan\n",
    "import pystan_utils\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.infer import config_enumerate, MCMC, NUTS, HMC, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# fix random generator seed (for reproducibility of results)\n",
    "np.random.seed(42)\n",
    "\n",
    "# matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>individual</th>\n",
       "      <th>hinc</th>\n",
       "      <th>psize</th>\n",
       "      <th>ttme_air</th>\n",
       "      <th>invc_air</th>\n",
       "      <th>invt_air</th>\n",
       "      <th>gc_air</th>\n",
       "      <th>ttme_train</th>\n",
       "      <th>invc_train</th>\n",
       "      <th>invt_train</th>\n",
       "      <th>gc_train</th>\n",
       "      <th>ttme_bus</th>\n",
       "      <th>invc_bus</th>\n",
       "      <th>invt_bus</th>\n",
       "      <th>gc_bus</th>\n",
       "      <th>invc_car</th>\n",
       "      <th>invt_car</th>\n",
       "      <th>gc_car</th>\n",
       "      <th>mode_chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>592.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  individual  hinc  psize  ttme_air  invc_air  invt_air  gc_air  \\\n",
       "0           0        70.0  30.0    4.0      10.0      61.0      80.0    73.0   \n",
       "1           1         8.0  15.0    4.0      64.0      48.0     154.0    71.0   \n",
       "2           2        62.0  35.0    2.0      64.0      58.0      74.0    69.0   \n",
       "3           3        61.0  40.0    3.0      45.0      75.0      75.0    96.0   \n",
       "4           4        27.0  70.0    1.0      20.0     106.0     190.0   127.0   \n",
       "\n",
       "   ttme_train  invc_train  invt_train  gc_train  ttme_bus  invc_bus  invt_bus  \\\n",
       "0        44.0        24.0       350.0      77.0      53.0      19.0     395.0   \n",
       "1        55.0        25.0       360.0      80.0      53.0      14.0     462.0   \n",
       "2        30.0        21.0       295.0      66.0      53.0      24.0     389.0   \n",
       "3        44.0        33.0       418.0      96.0      53.0      28.0     463.0   \n",
       "4        34.0        72.0       659.0     143.0      35.0      33.0     653.0   \n",
       "\n",
       "   gc_bus  invc_car  invt_car  gc_car  mode_chosen  \n",
       "0    79.0       4.0     314.0    52.0          1.0  \n",
       "1    84.0       4.0     351.0    57.0          2.0  \n",
       "2    83.0       7.0     315.0    55.0          2.0  \n",
       "3    98.0       5.0     291.0    49.0          1.0  \n",
       "4   104.0      44.0     592.0   108.0          1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv\n",
    "df = pd.read_csv(\"modechoice_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394, 17)\n",
      "(394,)\n",
      "(394,)\n"
     ]
    }
   ],
   "source": [
    "# separate between features/inputs (X) and target/output variables (y)\n",
    "mat = df.values\n",
    "X = mat[:,2:-1]\n",
    "print(X.shape)\n",
    "y = mat[:,-1].astype(\"int\")\n",
    "print(y.shape)\n",
    "ind = mat[:,1].astype(\"int\")\n",
    "print(ind.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is important!\n",
    "\n",
    "This is where we turn our previous 4-class problem into a binary classification problem: car vs non-car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to binary problem: car vs non-car\n",
    "y = (y == 4).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize input features\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 260\n",
      "num test: 134\n"
     ]
    }
   ],
   "source": [
    "train_perc = 0.66 # percentage of training data\n",
    "split_point = int(train_perc*len(y))\n",
    "perm = np.random.permutation(len(y))\n",
    "ix_train = perm[:split_point]\n",
    "ix_test = perm[split_point:]\n",
    "X_train = X[ix_train,:]\n",
    "X_test = X[ix_test,:]\n",
    "y_train = y[ix_train]\n",
    "y_test = y[ix_test]\n",
    "print(\"num train: %d\" % len(y_train))\n",
    "print(\"num test: %d\" % len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for the purpose of comparison, we run the logistic regression method from sklearn. But note that although sklearn has an implementation of logistic regression, it is not a Bayesian approach, nor does it support probit regression or some other variant that you may think is more appropriate for your particular problem. On the other hand, STAN offers us complete flexibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "true values: [1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
      " 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1]\n",
      "Accuracy: 0.7164179104477612\n"
     ]
    }
   ],
   "source": [
    "# create and fit logistic regression model\n",
    "logreg = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test set\n",
    "y_hat = logreg.predict(X_test)\n",
    "print(\"predictions:\", y_hat)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to implement binary logistic regression in STAN!\n",
    "\n",
    "Your turn now :-)\n",
    "\n",
    "Note: don't forget to include an explicit intercept parameter $\\alpha$ in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_54158a9c63a2718c46123f79ff152fa0 NOW.\n"
     ]
    }
   ],
   "source": [
    "# define Stan model\n",
    "model_definition = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;    // shape of data\n",
    "  int<lower=0> D;    // dimensions of x\n",
    "  matrix[N,D]  X;    // feature matrix\n",
    "  int          y[N]; // response vector\n",
    "}\n",
    "parameters {\n",
    "  real      alpha; // bias \n",
    "  vector[D] beta;  // parameters\n",
    "}\n",
    "model{\n",
    "  beta   ~ normal(0, 5);\n",
    "  alpha  ~ normal(0., 5);\n",
    "  \n",
    "  y ~ bernoulli_logit(alpha + X*beta);  \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# compile model\n",
    "sm = pystan.StanModel(model_code=model_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, obs=None):\n",
    "    n_beta = X.shape[1]\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 5.))     # Prior for the bias\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(0., 5.).expand([n_beta]).to_event(1)) # Priors for the regression coefficents\n",
    "    \n",
    "    mu = alpha + X.matmul(beta)\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=mu), obs=obs) # If you use logits you don't need to do sigmoid\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = 4\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1100/1100 [06:27,  2.84it/s, step size=2.35e-02, acc. prob=0.958]\n"
     ]
    }
   ],
   "source": [
    "#hmc_kernel = HMC(model)\n",
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=100, num_chains=1)\n",
    "mcmc.run(X_train, y_train) # Pyro accepts categories starting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "     alpha     -0.66      0.16     -0.66     -0.95     -0.42   1109.65      1.00\n",
      "   beta[0]      0.59      0.19      0.59      0.29      0.89   1048.14      1.00\n",
      "   beta[1]      0.24      0.28      0.23     -0.22      0.70   1026.03      1.00\n",
      "   beta[2]      0.63      0.17      0.62      0.37      0.91   1099.55      1.00\n",
      "   beta[3]      4.15      2.32      4.16      0.66      8.03    441.00      1.00\n",
      "   beta[4]      0.91      0.57      0.91     -0.02      1.77    517.49      1.00\n",
      "   beta[5]     -4.53      2.52     -4.55     -8.32     -0.44    436.13      1.00\n",
      "   beta[6]      0.19      0.18      0.19     -0.12      0.46    906.76      1.00\n",
      "   beta[7]     -0.15      1.37     -0.18     -2.23      2.34    499.85      1.01\n",
      "   beta[8]      3.52      1.81      3.46      0.58      6.20    486.14      1.00\n",
      "   beta[9]     -0.83      2.85     -0.70     -5.27      4.09    508.96      1.00\n",
      "  beta[10]      0.68      0.24      0.67      0.27      1.06    998.91      1.00\n",
      "  beta[11]      0.69      0.83      0.73     -0.63      2.01    455.30      1.00\n",
      "  beta[12]      3.69      1.70      3.73      0.92      6.45    410.75      1.00\n",
      "  beta[13]     -4.91      2.50     -4.97     -9.19     -0.91    414.18      1.00\n",
      "  beta[14]     -2.06      0.87     -2.04     -3.58     -0.68    428.45      1.00\n",
      "  beta[15]     -4.61      2.13     -4.69     -8.01     -0.96    488.13      1.00\n",
      "  beta[16]      3.43      2.49      3.54     -1.30      6.94    470.25      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input data for STAN, compile STAN program and run inference (MCMC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=260, D=17, C=1\n"
     ]
    }
   ],
   "source": [
    "# prepare data for Stan model\n",
    "N, D = X_train.shape\n",
    "print(\"N=%d, D=%d\" % (N,D))\n",
    "data = {'N': N, 'D': D, 'X': X_train, 'y': y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.6 ms, sys: 34.1 ms, total: 90.7 ms\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit = sm.sampling(data=data, iter=1000, chains=4, algorithm=\"NUTS\", seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_54158a9c63a2718c46123f79ff152fa0.\n",
      "4 chains, each with iter=1000; warmup=500; thin=1; \n",
      "post-warmup draws per chain=500, total post-warmup draws=2000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha     -0.67  3.5e-3   0.16  -0.97  -0.77  -0.67  -0.56  -0.36   2073    1.0\n",
      "beta[1]    0.59  4.1e-3    0.2   0.21   0.45   0.59   0.72    1.0   2427    1.0\n",
      "beta[2]    0.25  5.5e-3   0.29  -0.32   0.05   0.24   0.44   0.81   2762    1.0\n",
      "beta[3]    0.62  3.6e-3   0.17   0.29    0.5   0.62   0.73   0.96   2254    1.0\n",
      "beta[4]    3.94    0.07   2.26  -0.41   2.35   3.91   5.47   8.27   1116    1.0\n",
      "beta[5]    0.86    0.02   0.55  -0.23   0.48   0.86   1.22   1.97   1196    1.0\n",
      "beta[6]   -4.33    0.07   2.44   -9.1  -5.95  -4.31  -2.63   0.36   1121    1.0\n",
      "beta[7]    0.18  3.5e-3   0.17  -0.15   0.07   0.18   0.29   0.53   2381    1.0\n",
      "beta[8]   -0.29    0.04   1.46  -3.33  -1.27  -0.29   0.72   2.61   1073    1.0\n",
      "beta[9]    3.38    0.06    1.9  -0.44   2.15   3.38   4.67   6.96   1093    1.0\n",
      "beta[10]  -0.55    0.09   3.05  -6.57  -2.57  -0.59   1.46   5.61   1048    1.0\n",
      "beta[11]   0.66  5.2e-3   0.25   0.18   0.48   0.66   0.82    1.2   2388    1.0\n",
      "beta[12]   0.72    0.02   0.86  -0.97   0.16   0.73   1.28   2.38   1289    1.0\n",
      "beta[13]   3.77    0.05   1.78   0.25    2.6   3.77   4.96   7.43   1138    1.0\n",
      "beta[14]  -5.02    0.08   2.61  -9.94  -6.73  -5.04  -3.28   0.13   1060    1.0\n",
      "beta[15]   -2.0    0.03    0.9  -3.81   -2.6  -1.98  -1.37  -0.25   1058    1.0\n",
      "beta[16]  -4.51    0.07    2.1  -8.74  -5.92  -4.53  -3.09  -0.38   1003    1.0\n",
      "beta[17]   3.27    0.08   2.48  -1.78   1.65    3.3   4.94   8.13   1000    1.0\n",
      "lp__     -147.2    0.11   3.06 -154.4 -149.0 -146.9 -145.0 -142.2    779    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Fri Mar 15 09:39:24 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract samples from posterior, make predictions and compute accuracy (make sure that you understand all the code!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fit.extract(permuted=True)  # return a dictionary of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      "true values: [1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
      " 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1]\n",
      "Accuracy: 0.7313432835820896\n"
     ]
    }
   ],
   "source": [
    "# make predictions for test set\n",
    "mu = np.mean(samples[\"alpha\"].T + np.dot(X_test, samples[\"beta\"].T), axis=1)\n",
    "y_hat = (mu > 0).astype(\"int\")\n",
    "print(\"predictions:\", y_hat)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it seems that we are already doing better than sklearn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now lets try a **probit regression model in STAN**.\n",
    "\n",
    "Can you implement it?\n",
    "\n",
    "Note: the function that you need to use for the probit is called *Phi_approx* in STAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_f920a7b1d80ae11b599f4fcc0f87fdb8 NOW.\n"
     ]
    }
   ],
   "source": [
    "# define Stan model\n",
    "model_definition = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;    // shape of data\n",
    "  int<lower=0> D;    // dimensions of x\n",
    "  int<lower=0> C;    // number of possible choices\n",
    "  matrix[N,D]  X;    // feature matrix\n",
    "  int          y[N]; // response vector\n",
    "}\n",
    "parameters {\n",
    "  real      alpha; // bias \n",
    "  vector[D] beta;  // parameters\n",
    "}\n",
    "model{\n",
    "  vector[N] mu;\n",
    "  \n",
    "  beta   ~ normal(0, 5);\n",
    "  alpha  ~ normal(0, 5);\n",
    "  \n",
    "  mu = alpha + X*beta; \n",
    "  for (n in 1:N) {\n",
    "    mu[n] <- Phi_approx(mu[n]);\n",
    "  }\n",
    "  \n",
    "  y ~ bernoulli(mu);  \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# compile model\n",
    "sm = pystan.StanModel(model_code=model_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input data for STAN, compile STAN program and run inference (MCMC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=260, D=17, C=1\n"
     ]
    }
   ],
   "source": [
    "# prepare data for Stan model\n",
    "N, D = X_train.shape\n",
    "print(\"N=%d, D=%d\" % (N,D))\n",
    "data = {'N': N, 'D': D, 'X': X_train, 'y': y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.2 ms, sys: 28.6 ms, total: 98.7 ms\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit = sm.sampling(data=data, iter=1000, chains=4, algorithm=\"NUTS\", seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_f920a7b1d80ae11b599f4fcc0f87fdb8.\n",
      "4 chains, each with iter=1000; warmup=500; thin=1; \n",
      "post-warmup draws per chain=500, total post-warmup draws=2000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha     -0.37  1.9e-3    0.1  -0.56  -0.43  -0.37   -0.3  -0.19   2558    1.0\n",
      "beta[1]    0.37  2.5e-3   0.12   0.14   0.28   0.37   0.45    0.6   2435    1.0\n",
      "beta[2]    0.17  3.4e-3   0.17  -0.18   0.05   0.17   0.29    0.5   2712    1.0\n",
      "beta[3]    0.39  2.0e-3   0.11   0.18   0.31   0.39   0.46    0.6   2859    1.0\n",
      "beta[4]    3.39    0.05   1.71   0.14   2.21   3.34   4.56   6.76   1132    1.0\n",
      "beta[5]    0.74    0.01    0.4-9.1e-3   0.46   0.74    1.0   1.54   1181    1.0\n",
      "beta[6]   -3.66    0.05   1.86  -7.33  -4.94  -3.62  -2.34  -0.14   1141    1.0\n",
      "beta[7]    0.11  2.1e-3   0.11   -0.1   0.05   0.11   0.18   0.33   2647    1.0\n",
      "beta[8]   -0.08    0.05   1.29  -2.63  -0.95  -0.09   0.82   2.43    719    1.0\n",
      "beta[9]    2.26    0.06   1.61  -0.84   1.17   2.25   3.38   5.41    709    1.0\n",
      "beta[10]  -0.58     0.1   2.71  -5.83  -2.45  -0.58   1.28   4.86    698    1.0\n",
      "beta[11]   0.42  3.2e-3   0.15   0.13   0.32   0.41   0.52   0.73   2202    1.0\n",
      "beta[12]   0.76    0.02   0.65  -0.47    0.3   0.77   1.21   2.01    767    1.0\n",
      "beta[13]    3.1    0.05   1.39   0.43   2.14    3.1   4.06   5.77    733    1.0\n",
      "beta[14]  -4.24    0.08   2.08  -8.13  -5.68  -4.21  -2.83  -0.17    726    1.0\n",
      "beta[15]  -1.82    0.02   0.66  -3.07  -2.27  -1.82  -1.35  -0.52    895    1.0\n",
      "beta[16]  -4.22    0.05    1.6  -7.27  -5.34  -4.22  -3.14   -1.1    869    1.0\n",
      "beta[17]   3.82    0.06   1.87   0.13   2.54   3.86   5.09   7.37    880    1.0\n",
      "lp__     -145.3    0.11   3.09 -152.2 -147.1 -144.9 -143.0 -140.5    828    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Fri Mar 15 09:44:16 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract samples from posterior, make predictions and compute accuracy (make sure that you understand all the code!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fit.extract(permuted=True)  # return a dictionary of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      "true values: [1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
      " 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1]\n",
      "Accuracy: 0.7164179104477612\n"
     ]
    }
   ],
   "source": [
    "# make predictions for test set\n",
    "mu = np.mean(samples[\"alpha\"].T + np.dot(X_test, samples[\"beta\"].T), axis=1)\n",
    "y_hat = (mu > 0).astype(\"int\")\n",
    "print(\"predictions:\", y_hat)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are your results in comparison to the version with the logistic sigmoid?\n",
    "\n",
    "In some cases, using a probit function instead of the logistic sigmoid can make a significant difference. In other cases, it doesn't... You have to consider what makes more sense to the specific problem that you are trying to solve. Or, we can just try different approaches! That is just fine... STAN makes it very easy to try all these different variants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
