{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09 - Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome back! Today, we'll work on a very useful text analysis technique called Topic Modeling. Particularly, we will use its most popular and powerful algorithm: Latent Dirichlet Allocation, or LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you implement your own LDA on STAN, it is important that you understand all concepts well. In this notebook, we'll start by playing a little bit with the Dirichlet distribution. After this, we'll ask you to do some ancestral sampling on the LDA generative story.\n",
    "\n",
    "Only after that, you'll do your own LDA in STAN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by the usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pystan_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding the dirichlet distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution is available as numpy.random.dirichlet(alpha, size=None)\n",
    "\n",
    "...so, try it! For example, obtain draws from this distribution using different values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.77222756e-01 2.22680163e-01 9.70815870e-05]\n",
      "[1.04820976e-03 2.26730018e-05 9.98929117e-01]\n",
      "[0.13245564 0.52424567 0.34329869]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.dirichlet([.2,.2, .2]))\n",
    "print(np.random.dirichlet([.1,.1, .9]))\n",
    "print(np.random.dirichlet([1,1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the sum is always 1, for all vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.random.dirichlet([.2,.2, .2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you can, try to visualize it. Remember what we did in the slides. Try to do the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feel free to use the function below, to plot points from a dirichlet distribution, onto a 2D simplex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to plot points in a simplex'''\n",
    "\n",
    "# Based on post from Thomas Boggs (http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)\n",
    "\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "_corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "_triangle = tri.Triangulation(_corners[:, 0], _corners[:, 1])\n",
    "\n",
    "def plot_points(X):\n",
    "    '''Plots a set of points in the simplex.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `X` (ndarray): A 2xN array (if in Cartesian coords) or 3xN array\n",
    "                       (if in barycentric coords) of points to plot.\n",
    "    '''\n",
    "    \n",
    "    X = X.dot(_corners)  #This is what converts the original points onto the simplex (it projects on it, through dot product)\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', ms=1)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 0.75**0.5)\n",
    "    plt.axis('off')\n",
    "    plt.triplot(_triangle, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10000 points from the dirichlet distribution and plot them using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.dirichlet([.09, .09, .09], size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_points(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with different values of $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\alpha = [1,1,1]$ corresponds to a uniform distribution. Values of $\\alpha < 1$ lead to sparse distributions (can be used as sparsity-inducing priors!). The higher the value of $\\alpha$, the more concentrated is the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LDA Ancestral Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our own dictionary of 30 words. There are 3 different topics that we embedded in the dictionary, can you see what they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "dictionary =[\"Copenhagen\",\n",
    "             \"Madrid\",\n",
    "             \"Sydney\",\n",
    "             \"Kabul\",\n",
    "             \"Vienna\",\n",
    "             \"Brussels\",\n",
    "             \"Beijing\",\n",
    "             \"Kathmandu\",\n",
    "             \"Singapore\",\n",
    "             \"Oslo\",\n",
    "             \"blue\",\n",
    "             \"green\",\n",
    "             \"beige\",\n",
    "             \"cyan\",\n",
    "             \"black\",\n",
    "             \"tan\",\n",
    "             \"brown\",\n",
    "             \"orange\",\n",
    "             \"white\",\n",
    "             \"red\",\n",
    "             \"data\",\n",
    "             \"model\",\n",
    "             \"inference\",\n",
    "             \"learning\",\n",
    "             \"observation\",\n",
    "             \"dimension\",\n",
    "             \"training\",\n",
    "             \"neuralnetwork\",\n",
    "             \"analytics\",\n",
    "             \"sampling\"\n",
    "             ]\n",
    "C = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it smoother and more fun, we'll start by giving you the topics, $\\phi$ and the proportions $\\theta$ directly. I.e., you don't need to generate (yet) those from Dirichlet distribution.\n",
    "\n",
    "Lets start by defining the 3 topics. We do this by assigning a probability for each word under of the 3 topics ($\\phi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define word vectors (for each topic) and normalize:\n",
    "phi = np.zeros( (K, C) );\n",
    "phi[0] = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "phi[0] *= 1/np.sum(phi[0])\n",
    "phi[1] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "phi[1] *= 1/np.sum(phi[1])\n",
    "phi[2] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "phi[2] *= 1/np.sum(phi[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for topic 1, we assigned high probability to the words: Copenhagen, Madrid, Sydney, Kabul, Vienna, Brussels, Beijing, Kathmandu, Singapore, Oslo. \n",
    "\n",
    "Similarly, for topic 2, we assigned high probability to the words: blue, green, ...\n",
    "\n",
    "Finaly, for topic 3, we assigned high probability to the words: model, inference, ...\n",
    "\n",
    "Now that we have defined the probability of each word under each topic, we can follow the generative process to sample some documents. As previously mentioned, we will pre-define the distribution over topics for each document ($\\theta$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 3 ## Number of documents.\n",
    "\n",
    "## Define topic proportion vectors (one for for each document):\n",
    "theta = np.zeros( (I, K) );\n",
    "theta[0] = [0.6, 0.2, 0.2]\n",
    "theta[1] = [1/3, 1/3, 1/3] ## This one will be an equal mix of all topics\n",
    "theta[2] = [  0,   0,  1] ## This will for example only contain topic 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first document has higher probability of containing words from topic 1 than topics 2 and 3. Document 2 is uniform - all topics have equal probability. Document 3 covers exclusevely topic 3.\n",
    "\n",
    "Let us now generate the 3 documents following the generative process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you assume now that we have $\\boldsymbol{\\theta}_i$ for all documents and $\\boldsymbol{\\phi}_k$ for all topics. Ancestral sampling is simply:\n",
    "\n",
    "\n",
    "For each document $i$, and for each word $j=1...w_i$, do:\n",
    "\\begin{align}\n",
    "z_{i, j} &\\sim Cat(\\boldsymbol{\\theta}_i) \\\\\n",
    "w_{i, j} &\\sim Cat(\\boldsymbol{\\phi}_{z_{i,j}})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you try code it down yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Just to make things practical, we created this function for you\n",
    "## just read it or, try it, to see what it does...\n",
    "def categorical_sample(p):\n",
    "    return list(np.random.multinomial(1, p)).index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that receives document sizes J, theta (topic proportions) and phi (topics), \n",
    "## returns z (word-topic assignments ) and w (words)\n",
    "def ancestral_sampling(J, theta, phi):\n",
    "    ## Initialise\n",
    "    z = np.zeros( (I, np.max(J)), dtype=int )  #NOTICE that z and w are vectors of integers!\n",
    "    w = np.zeros( (I, np.max(J)), dtype=int )  \n",
    "\n",
    "    '''\n",
    "    TODO: write your code here\n",
    "    '''\n",
    "    for i in range(I):\n",
    "        for j in range(J[i]):\n",
    "            z[i,j] = categorical_sample(theta[i])\n",
    "            w[i,j] = categorical_sample(phi[z[i,j]])\n",
    "    return z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ 5  1 13 21 26 18  0  6 23  6  8  3]\n",
      " [16  1 27 26 19 29 26  6  8  0  0  0]\n",
      " [23 26 24 29 29 23 29 21 23 26  0  0]]\n",
      "\n",
      "\n",
      "Document:  0\n",
      "Theta:  [0.6 0.2 0.2]\n",
      "Word 0: Topic assignment: 1->   Brussels  \n",
      "Word 1: Topic assignment: 1->   Madrid  \n",
      "Word 2: Topic assignment: 2->   cyan  \n",
      "Word 3: Topic assignment: 3->   model  \n",
      "Word 4: Topic assignment: 3->   training  \n",
      "Word 5: Topic assignment: 2->   white  \n",
      "Word 6: Topic assignment: 1->   Copenhagen  \n",
      "Word 7: Topic assignment: 1->   Beijing  \n",
      "Word 8: Topic assignment: 3->   learning  \n",
      "Word 9: Topic assignment: 1->   Beijing  \n",
      "Word 10: Topic assignment: 1->   Singapore  \n",
      "Word 11: Topic assignment: 1->   Kabul  \n",
      "\n",
      "\n",
      "Document:  1\n",
      "Theta:  [0.33333333 0.33333333 0.33333333]\n",
      "Word 0: Topic assignment: 2->   brown  \n",
      "Word 1: Topic assignment: 1->   Madrid  \n",
      "Word 2: Topic assignment: 3->   neuralnetwork  \n",
      "Word 3: Topic assignment: 3->   training  \n",
      "Word 4: Topic assignment: 2->   red  \n",
      "Word 5: Topic assignment: 3->   sampling  \n",
      "Word 6: Topic assignment: 3->   training  \n",
      "Word 7: Topic assignment: 1->   Beijing  \n",
      "Word 8: Topic assignment: 1->   Singapore  \n",
      "\n",
      "\n",
      "Document:  2\n",
      "Theta:  [0. 0. 1.]\n",
      "Word 0: Topic assignment: 3->   learning  \n",
      "Word 1: Topic assignment: 3->   training  \n",
      "Word 2: Topic assignment: 3->   observation  \n",
      "Word 3: Topic assignment: 3->   sampling  \n",
      "Word 4: Topic assignment: 3->   sampling  \n",
      "Word 5: Topic assignment: 3->   learning  \n",
      "Word 6: Topic assignment: 3->   sampling  \n",
      "Word 7: Topic assignment: 3->   model  \n",
      "Word 8: Topic assignment: 3->   learning  \n",
      "Word 9: Topic assignment: 3->   training  \n"
     ]
    }
   ],
   "source": [
    "J = [12, 9, 10] ## Vector of size I denoting how many words are in each document\n",
    "z, w = ancestral_sampling(J, theta, phi)\n",
    "print(\"w:\", w)\n",
    "for i in range(I):\n",
    "    print(\"\\n\\nDocument: \", i)\n",
    "    print(\"Theta: \", theta[i])\n",
    "    for j in range(J[i]):\n",
    "        print(\"Word %d: Topic assignment: %d->   %s  \" % (j,  z[i, j]+1, dictionary[w[i, j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to take a look at the generated documents? Do they make sense, given the values for $\\phi$ and $\\theta$ given above?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bet you guessed what we want you to do next: having generated yourself the data, can you make an LDA model in STAN that recovers the original parameters $\\phi$ and $\\theta$?\n",
    "\n",
    "A dataset with only 3 documents is too little of course. So, let's generate 100 documents intead, by using the dirichlet distribution. In other words, we will generate:\n",
    "- vectors $\\theta_1 \\dots \\theta_I$\n",
    "- a new vector $J$ that contains their sizes, with $J_i \\sim Poisson(10)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=100\n",
    "theta = np.zeros((I, K));\n",
    "alpha = 0.5*np.ones(K) ## Size K dirichlet prior\n",
    "J=[]\n",
    "\n",
    "for i in range(I):\n",
    "    theta[i] = np.random.dirichlet(alpha);\n",
    "    #J.append(int(np.random.poisson(10)))\n",
    "    J.append(10)\n",
    "_, W = ancestral_sampling(J, theta, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please reuse your code to generate the dataset W. Please reuse $\\phi$ as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implement your own LDA model in STAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to make your own model in STAN! :-) Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import constraints\n",
    "import functools\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, AutoGuideList, AutoDelta\n",
    "from pyro.optim import ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fully generative model of a batch of documents.\n",
    "# data is a [num_words_per_doc, num_documents] shaped array of word ids\n",
    "# (specifically it is not a histogram). We assume in this simple example\n",
    "# that all documents have the same number of words.\n",
    "\n",
    "num_words = C\n",
    "num_topics = K\n",
    "num_docs = I\n",
    "num_words_per_doc = 10\n",
    "\n",
    "#num_words = 1024\n",
    "#num_topics = 8\n",
    "#num_docs = 1000\n",
    "#num_words_per_doc = 64\n",
    "\n",
    "def model(data=None, batch_size=None):\n",
    "    # Globals.\n",
    "    with pyro.plate(\"topics\", num_topics):\n",
    "        topic_weights = pyro.sample(\"topic_weights\", dist.Gamma(1. / num_topics, 1.))\n",
    "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(num_words) / num_words))\n",
    "\n",
    "    # Locals.\n",
    "    with pyro.plate(\"documents\", num_docs) as ind:\n",
    "        if data is not None:\n",
    "            with pyro.util.ignore_jit_warnings():\n",
    "                assert data.shape == (num_words_per_doc, num_docs)\n",
    "            data = data[:, ind]\n",
    "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(topic_weights))\n",
    "        with pyro.plate(\"words\", num_words_per_doc):\n",
    "            # The word_topics variable is marginalized out during inference,\n",
    "            # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
    "            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
    "            # the guide.\n",
    "            word_topics = pyro.sample(\"word_topics\", dist.Categorical(doc_topics), infer={\"enumerate\": \"parallel\"})\n",
    "            data = pyro.sample(\"doc_words\", dist.Categorical(topic_words[word_topics]), obs=data)\n",
    "\n",
    "    return topic_weights, topic_words, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can generate synthetic data directly by calling the model.\n",
    "#true_topic_weights, true_topic_words, fake_data = model()\n",
    "#fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W_torch = torch.tensor(W.T).float()\n",
    "W_torch = torch.tensor(W.T).long()\n",
    "W_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual+Autoguide version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 3704.9\n",
      "[100] ELBO: 3859.5\n",
      "[200] ELBO: 3681.0\n",
      "[300] ELBO: 3819.5\n",
      "[400] ELBO: 3650.1\n",
      "[500] ELBO: 3599.3\n",
      "[600] ELBO: 3534.3\n",
      "[700] ELBO: 3656.3\n",
      "[800] ELBO: 3650.6\n",
      "[900] ELBO: 3531.0\n",
      "[1000] ELBO: 3594.0\n",
      "[1100] ELBO: 3464.4\n",
      "[1200] ELBO: 3470.7\n",
      "[1300] ELBO: 3498.8\n",
      "[1400] ELBO: 3417.2\n",
      "[1500] ELBO: 3443.1\n",
      "[1600] ELBO: 3426.2\n",
      "[1700] ELBO: 3495.3\n",
      "[1800] ELBO: 3459.1\n",
      "[1900] ELBO: 3493.7\n",
      "[2000] ELBO: 3415.1\n",
      "[2100] ELBO: 3452.0\n",
      "[2200] ELBO: 3332.0\n",
      "[2300] ELBO: 3466.3\n",
      "[2400] ELBO: 3436.6\n",
      "[2500] ELBO: 3250.2\n",
      "[2600] ELBO: 3302.0\n",
      "[2700] ELBO: 3351.9\n",
      "[2800] ELBO: 3336.0\n",
      "[2900] ELBO: 3701.1\n",
      "[3000] ELBO: 3399.0\n",
      "[3100] ELBO: 3385.6\n",
      "[3200] ELBO: 3274.3\n",
      "[3300] ELBO: 3416.6\n",
      "[3400] ELBO: 3511.1\n",
      "[3500] ELBO: 3204.3\n",
      "[3600] ELBO: 3207.8\n",
      "[3700] ELBO: 3199.8\n",
      "[3800] ELBO: 3184.2\n",
      "[3900] ELBO: 3374.2\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#guide = AutoDiagonalNormal(pyro.poutine.block(model, hide=[\"word_topics\", \"doc_words\"]))\n",
    "\n",
    "def my_local_guide(data=None, batch_size=None):\n",
    "    # Use a conjugate guide for global variables.\n",
    "    topic_weights_posterior = pyro.param(\n",
    "            \"topic_weights_posterior\",\n",
    "            lambda: torch.ones(num_topics),\n",
    "            constraint=constraints.positive)\n",
    "    topic_words_posterior = pyro.param(\n",
    "            \"topic_words_posterior\",\n",
    "            lambda: torch.ones(num_topics, num_words),\n",
    "            constraint=constraints.positive)\n",
    "            #constraint=constraints.greater_than(0.5))\n",
    "            #constraint=constraints.simplex)\n",
    "    with pyro.plate(\"topics\", num_topics):\n",
    "        pyro.sample(\"topic_weights\", dist.Gamma(topic_weights_posterior, 1.))\n",
    "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
    "    \n",
    "    doc_topics_posterior = pyro.param(\n",
    "            \"doc_topics_posterior\",\n",
    "            lambda: torch.ones(num_docs, num_topics),\n",
    "            constraint=constraints.simplex)\n",
    "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
    "        #print(doc_topics_posterior.shape)\n",
    "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics_posterior[ind], event_dim=1))\n",
    "    \n",
    "guide = AutoGuideList(model)\n",
    "guide.add(AutoDiagonalNormal(pyro.poutine.block(model, expose=['doc_topics'])))\n",
    "guide.add(my_local_guide)  # automatically wrapped in an AutoCallable\n",
    "\n",
    "guide = my_local_guide\n",
    "\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
    "\n",
    "optim = ClippedAdam({'lr': 0.005})\n",
    "svi = SVI(model, guide, optim, elbo)\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 4000\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(W_torch, batch_size=32)\n",
    "    if step % 100 == 0:\n",
    "        #print('.', end='')\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"doc_topics\",))\n",
    "samples = predictive(W_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_posterior = samples[\"doc_topics\"].mean(axis=0)\n",
    "doc_topics_posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 1, 2, 0, 2, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0,\n",
       "        1, 0, 0, 2, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 2, 2, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 1, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(doc_topics_posterior, axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 1, 2, 2, 2, 0,\n",
       "       1, 2, 2, 1, 0, 0, 2, 1, 0, 2, 1, 2, 2, 2, 0, 2, 1, 1, 1, 2, 0, 2,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 2, 1, 0, 2, 1, 2, 0, 2, 2, 0, 2, 1,\n",
       "       2, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0,\n",
       "       1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(theta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(torch.argmax(doc_topics_posterior, axis=2)[0].detach().numpy() == np.argmax(theta, axis=1)) / len(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"topic_weights\",))\n",
    "samples = predictive(W_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3421, 0.3371, 0.3208]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_weights_posterior = samples[\"topic_weights\"].mean(axis=0)\n",
    "topic_weights_posterior / topic_weights_posterior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 1, 3, 30])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"topic_words\",))\n",
    "samples = predictive(W_torch)\n",
    "samples[\"topic_words\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"topic_words\"].mean(axis=0)[0].argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoguide version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 4722.5\n",
      "[100] ELBO: 4473.0\n",
      "[200] ELBO: 4212.2\n",
      "[300] ELBO: 3940.5\n",
      "[400] ELBO: 3717.8\n",
      "[500] ELBO: 3563.4\n",
      "[600] ELBO: 3485.7\n",
      "[700] ELBO: 3461.4\n",
      "[800] ELBO: 3451.2\n",
      "[900] ELBO: 3429.2\n",
      "[1000] ELBO: 3419.5\n",
      "[1100] ELBO: 3413.0\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "guide = AutoDiagonalNormal(pyro.poutine.block(model, hide=[\"word_topics\", \"doc_words\"]))\n",
    "\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
    "\n",
    "optim = ClippedAdam({'lr': 0.005})\n",
    "svi = SVI(model, guide, optim, elbo)\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 1200\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(W_torch, batch_size=32)\n",
    "    if step % 100 == 0:\n",
    "        #print('.', end='')\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"doc_topics\",))\n",
    "samples = predictive(W_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_posterior = samples[\"doc_topics\"].mean(axis=0)\n",
    "doc_topics_posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 2, 2, 2, 1, 2, 0, 1, 1, 0, 2, 0, 2, 0, 1, 1, 0, 1, 2, 0, 1, 2,\n",
       "        1, 1, 1, 1, 2, 2, 0, 1, 1, 0, 2, 0, 2, 2, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1,\n",
       "        1, 2, 2, 2, 1, 1, 0, 0, 2, 2, 0, 1, 0, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 2,\n",
       "        0, 1, 2, 0, 1, 0, 2, 0, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 2, 0, 1, 1,\n",
       "        2, 0, 2, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(doc_topics_posterior, axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 2, 0, 1, 2, 0, 0, 2, 0,\n",
       "       1, 2, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 0, 0, 2, 2, 1, 2, 0, 1, 2,\n",
       "       0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0, 2, 2, 0, 1, 0, 2, 1, 2, 0, 2,\n",
       "       2, 0, 2, 1, 1, 2, 0, 1, 1, 0, 0, 0, 2, 1, 2, 2, 2, 2, 2, 1, 0, 1,\n",
       "       2, 1, 0, 0, 2, 0, 1, 1, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(theta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(torch.argmax(doc_topics_posterior, axis=2)[0].detach().numpy() == np.argmax(theta, axis=1)) / len(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"topic_weights\",))\n",
    "samples = predictive(W_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3287, 0.3137, 0.3576]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_weights_posterior = samples[\"topic_weights\"].mean(axis=0)\n",
    "topic_weights_posterior / topic_weights_posterior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 1, 3, 30])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"topic_words\",))\n",
    "samples = predictive(W_torch)\n",
    "samples[\"topic_words\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"topic_words\"].mean(axis=0)[0].argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use amortized inference of the local topic variables, achieved by a\n",
    "# multi-layer perceptron. We'll wrap the guide in an nn.Module.\n",
    "\n",
    "def make_predictor():\n",
    "    layer_sizes = \"100-100\"\n",
    "    layer_sizes = ([num_words] +\n",
    "                   [int(s) for s in layer_sizes.split('-')] +\n",
    "                   [num_topics])\n",
    "    print('Creating MLP with sizes {}'.format(layer_sizes))\n",
    "    layers = []\n",
    "    for in_size, out_size in zip(layer_sizes, layer_sizes[1:]):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "        layer.weight.data.normal_(0, 0.001)\n",
    "        layer.bias.data.normal_(0, 0.001)\n",
    "        layers.append(layer)\n",
    "        layers.append(nn.Sigmoid())\n",
    "    layers.append(nn.Softmax(dim=-1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def parametrized_guide(predictor, data, batch_size=None):\n",
    "    # Use a conjugate guide for global variables.\n",
    "    topic_weights_posterior = pyro.param(\n",
    "            \"topic_weights_posterior\",\n",
    "            lambda: torch.ones(num_topics),\n",
    "            constraint=constraints.positive)\n",
    "    topic_words_posterior = pyro.param(\n",
    "            \"topic_words_posterior\",\n",
    "            lambda: torch.ones(num_topics, num_words),\n",
    "            constraint=constraints.greater_than(0.5))\n",
    "    with pyro.plate(\"topics\", num_topics):\n",
    "        pyro.sample(\"topic_weights\", dist.Gamma(topic_weights_posterior, 1.))\n",
    "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
    "\n",
    "    # Use an amortized guide for local variables.\n",
    "    pyro.module(\"predictor\", predictor)\n",
    "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
    "        data = data[:, ind]\n",
    "        # The neural network will operate on histograms rather than word\n",
    "        # index vectors, so we'll convert the raw data to a histogram.\n",
    "        counts = (torch.zeros(num_words, ind.size(0))\n",
    "                       .scatter_add(0, data, torch.ones(data.shape)))\n",
    "        doc_topics = predictor(counts.transpose(0, 1))\n",
    "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics, event_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MLP with sizes [30, 100, 100, 3]\n",
      "[0] ELBO: 3837.7\n",
      "[100] ELBO: 3849.8\n",
      "[200] ELBO: 3923.1\n",
      "[300] ELBO: 3865.1\n",
      "[400] ELBO: 3669.2\n",
      "[500] ELBO: 3590.2\n",
      "[600] ELBO: 3581.5\n",
      "[700] ELBO: 3505.0\n",
      "[800] ELBO: 3767.5\n",
      "[900] ELBO: 3603.6\n",
      "[1000] ELBO: 3483.0\n",
      "[1100] ELBO: 3478.2\n",
      "[1200] ELBO: 3803.9\n",
      "[1300] ELBO: 3608.5\n",
      "[1400] ELBO: 3461.3\n",
      "[1500] ELBO: 3509.1\n",
      "[1600] ELBO: 3392.6\n",
      "[1700] ELBO: 3632.0\n",
      "[1800] ELBO: 3403.5\n",
      "[1900] ELBO: 3493.9\n",
      "[2000] ELBO: 3473.0\n",
      "[2100] ELBO: 3445.7\n",
      "[2200] ELBO: 3466.4\n",
      "[2300] ELBO: 3453.6\n",
      "[2400] ELBO: 3466.6\n",
      "[2500] ELBO: 3485.4\n",
      "[2600] ELBO: 3411.6\n",
      "[2700] ELBO: 3481.9\n",
      "[2800] ELBO: 3438.1\n",
      "[2900] ELBO: 3411.2\n",
      "[3000] ELBO: 3390.1\n",
      "[3100] ELBO: 3326.8\n",
      "[3200] ELBO: 3401.2\n",
      "[3300] ELBO: 3402.4\n",
      "[3400] ELBO: 3523.6\n",
      "[3500] ELBO: 3395.0\n",
      "[3600] ELBO: 3369.4\n",
      "[3700] ELBO: 3381.3\n",
      "[3800] ELBO: 3455.6\n",
      "[3900] ELBO: 3334.1\n",
      "[4000] ELBO: 3415.4\n",
      "[4100] ELBO: 3497.3\n",
      "[4200] ELBO: 3428.1\n",
      "[4300] ELBO: 3463.2\n",
      "[4400] ELBO: 3317.4\n",
      "[4500] ELBO: 3426.8\n",
      "[4600] ELBO: 3426.4\n",
      "[4700] ELBO: 3396.7\n",
      "[4800] ELBO: 3939.1\n",
      "[4900] ELBO: 3432.7\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 5000\n",
    "\n",
    "predictor = make_predictor()\n",
    "guide = functools.partial(parametrized_guide, predictor)\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
    "optim = ClippedAdam({'lr': 0.005})\n",
    "svi = SVI(model, guide, optim, elbo)\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(W_torch, batch_size=32)\n",
    "    if step % 100 == 0:\n",
    "        #print('.', end='')\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (torch.zeros(num_words, W_torch.size(1))\n",
    "                       .scatter_add(0, W_torch, torch.ones(W_torch.shape)))\n",
    "doc_topics = predictor(counts.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"doc_topics\",))\n",
    "samples = predictive(W_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics_posterior = samples[\"doc_topics\"].mean(axis=0)\n",
    "doc_topics_posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(doc_topics_posterior, axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 2, 0, 1, 2, 0, 0, 2, 0,\n",
       "       1, 2, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 0, 0, 2, 2, 1, 2, 0, 1, 2,\n",
       "       0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0, 2, 2, 0, 1, 0, 2, 1, 2, 0, 2,\n",
       "       2, 0, 2, 1, 1, 2, 0, 1, 1, 0, 0, 0, 2, 1, 2, 2, 2, 2, 2, 1, 0, 1,\n",
       "       2, 1, 0, 0, 2, 0, 1, 1, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(theta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(torch.argmax(doc_topics_posterior, axis=2)[0].detach().numpy() == np.argmax(theta, axis=1)) / len(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"topic_weights\",))\n",
    "samples = predictive(W_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3208, 0.3188, 0.3604]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_weights_posterior = samples[\"topic_weights\"].mean(axis=0)\n",
    "topic_weights_posterior / topic_weights_posterior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 1, 3, 30])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"topic_words\",))\n",
    "samples = predictive(W_torch)\n",
    "samples[\"topic_words\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1, 0, 1, 0, 0, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 1, 2, 1, 0, 1, 0, 1,\n",
       "        2, 1, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"topic_words\"].mean(axis=0)[0].argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAN version (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_16f46dd087fdbdd3cc0497c194130279 NOW.\n"
     ]
    }
   ],
   "source": [
    "LDA_STAN=\"\"\"\n",
    "data{\n",
    "  int<lower=0> I;           // Number of documents\n",
    "  int<lower=0> J[I];        // Number of words in each document\n",
    "  int<lower=0> MAX_J;       // Max number of words in all the documents\n",
    "  int<lower=0> K;           // Number of topics\n",
    "  int<lower=0> C;           // Dictionary size (Total number of words)\n",
    "  vector[K]    alpha;       // Dirichlet prior for document's topic distribution (theta)\n",
    "  vector[C]    beta;        // Dirichlet prior for topic's word distribution\n",
    "  int<lower=0> W[I, MAX_J]; // Documents consisting of j words\n",
    "}\n",
    "parameters{\n",
    "  simplex[K] theta[I]; // Topic distribution in each document\n",
    "  simplex[C] phi[K];   // Distribution of words in each topic\n",
    "}\n",
    "model{\n",
    "  for (i in 1:I){\n",
    "    theta[i] ~ dirichlet(alpha);\n",
    "  }\n",
    "  for (k in 1:K){\n",
    "    phi[k]   ~ dirichlet(beta);\n",
    "  }\n",
    "  \n",
    "  for (i in 1:I){\n",
    "    for (j in 1:J[i]){\n",
    "      real gamma[K];\n",
    "      for (k in 1:K){\n",
    "        // log(p(z=k|theta)) + log(p(w|phi, z=k))\n",
    "        gamma[k] = log(theta[i,k]) + log(phi[k, W[i,j]]); // indexes of theta and phi change because they are defined as repetitions of rows(?)\n",
    "      }\n",
    "      target += log_sum_exp(gamma); // target is a protected variable to sum on the log likelihood\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "## Compile the model\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5*np.ones(C) ## Size C dirichlet prior, for topic word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/tmp/tmph3srj01s/output.csv`\n"
     ]
    }
   ],
   "source": [
    "## Collect the data\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':np.max(J), 'W':W+1}\n",
    "## Sample with VB\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract results from STAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = pystan_utils.vb_extract_variable(fit, \"theta\", var_type=\"matrix\", dims=[I,K])\n",
    "phi_hat = pystan_utils.vb_extract_variable(fit, \"phi\", var_type=\"matrix\", dims=[K,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare estimated $\\hat{\\phi}$ with true values of $\\phi$ used to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi_hat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything interesting? Do the results match exactly? Remember that if we call \"topic 1\" -> \"topic 2\", and we call \"topic 2\" -> \"topic 1\", the resulting model is still the same! We just \"renamed\" the topics. This is another case of the problem of model identifiability...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
