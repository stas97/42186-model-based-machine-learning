{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09 - Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome back! Today, we'll work on a very useful text analysis technique called Topic Modeling. Particularly, we will use its most popular and powerful algorithm: Latent Dirichlet Allocation, or LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you implement your own LDA on STAN, it is important that you understand all concepts well. In this notebook, we'll start by playing a little bit with the Dirichlet distribution. After this, we'll ask you to do some ancestral sampling on the LDA generative story.\n",
    "\n",
    "Only after that, you'll do your own LDA in STAN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by the usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pystan_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding the dirichlet distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution is available as numpy.random.dirichlet(alpha, size=None)\n",
    "\n",
    "...so, try it! For example, obtain draws from this distribution using different values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22073447 0.02829487 0.75097066]\n",
      "[9.58299928e-01 3.50907955e-09 4.17000682e-02]\n",
      "[0.3763067  0.06993453 0.55375878]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.dirichlet([.2,.2, .2]))\n",
    "print(np.random.dirichlet([.1,.1, .9]))\n",
    "print(np.random.dirichlet([1,1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the sum is always 1, for all vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you can, try to visualize it. Remember what we did in the slides. Try to do the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feel free to use the function below, to plot points from a dirichlet distribution, onto a 2D simplex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to plot points in a simplex'''\n",
    "\n",
    "# Based on post from Thomas Boggs (http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)\n",
    "\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "_corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "_triangle = tri.Triangulation(_corners[:, 0], _corners[:, 1])\n",
    "\n",
    "def plot_points(X):\n",
    "    '''Plots a set of points in the simplex.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `X` (ndarray): A 2xN array (if in Cartesian coords) or 3xN array\n",
    "                       (if in barycentric coords) of points to plot.\n",
    "    '''\n",
    "    \n",
    "    X = X.dot(_corners)  #This is what converts the original points onto the simplex (it projects on it, through dot product)\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', ms=1)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 0.75**0.5)\n",
    "    plt.axis('off')\n",
    "    plt.triplot(_triangle, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10000 points from the dirichlet distribution and plot them using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with different values of $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\alpha = [1,1,1]$ corresponds to a uniform distribution. Values of $\\alpha < 1$ lead to sparse distributions (can be used as sparsity-inducing priors!). The higher the value of $\\alpha$, the more concentrated is the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LDA Ancestral Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our own dictionary of 30 words. There are 3 different topics that we embedded in the dictionary, can you see what they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "dictionary =[\"Copenhagen\",\n",
    "             \"Madrid\",\n",
    "             \"Sydney\",\n",
    "             \"Kabul\",\n",
    "             \"Vienna\",\n",
    "             \"Brussels\",\n",
    "             \"Beijing\",\n",
    "             \"Kathmandu\",\n",
    "             \"Singapore\",\n",
    "             \"Oslo\",\n",
    "             \"blue\",\n",
    "             \"green\",\n",
    "             \"beige\",\n",
    "             \"cyan\",\n",
    "             \"black\",\n",
    "             \"tan\",\n",
    "             \"brown\",\n",
    "             \"orange\",\n",
    "             \"white\",\n",
    "             \"red\",\n",
    "             \"data\",\n",
    "             \"model\",\n",
    "             \"inference\",\n",
    "             \"learning\",\n",
    "             \"observation\",\n",
    "             \"dimension\",\n",
    "             \"training\",\n",
    "             \"neuralnetwork\",\n",
    "             \"analytics\",\n",
    "             \"sampling\"\n",
    "             ]\n",
    "C = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it smoother and more fun, we'll start by giving you the topics, $\\phi$ and the proportions $\\theta$ directly. I.e., you don't need to generate (yet) those from Dirichlet distribution.\n",
    "\n",
    "Lets start by defining the 3 topics. We do this by assigning a probability for each word under of the 3 topics ($\\phi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define word vectors (for each topic) and normalize:\n",
    "phi = np.zeros( (K, C) );\n",
    "phi[0] = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "phi[0] *= 1/np.sum(phi[0])\n",
    "phi[1] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "phi[1] *= 1/np.sum(phi[1])\n",
    "phi[2] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "phi[2] *= 1/np.sum(phi[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for topic 1, we assigned high probability to the words: Copenhagen, Madrid, Sydney, Kabul, Vienna, Brussels, Beijing, Kathmandu, Singapore, Oslo. \n",
    "\n",
    "Similarly, for topic 2, we assigned high probability to the words: blue, green, ...\n",
    "\n",
    "Finaly, for topic 3, we assigned high probability to the words: model, inference, ...\n",
    "\n",
    "Now that we have defined the probability of each word under each topic, we can follow the generative process to sample some documents. As previously mentioned, we will pre-define the distribution over topics for each document ($\\theta$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 3 ## Number of documents.\n",
    "\n",
    "## Define topic proportion vectors (one for for each document):\n",
    "theta = np.zeros( (I, K) );\n",
    "theta[0] = [0.6, 0.2, 0.2]\n",
    "theta[1] = [1/3, 1/3, 1/3] ## This one will be an equal mix of all topics\n",
    "theta[2] = [  0,   0,  1] ## This will for example only contain topic 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first document has higher probability of containing words from topic 1 than topics 2 and 3. Document 2 is uniform - all topics have equal probability. Document 3 covers exclusevely topic 3.\n",
    "\n",
    "Let us now generate the 3 documents following the generative process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you assume now that we have $\\boldsymbol{\\theta}_i$ for all documents and $\\boldsymbol{\\phi}_k$ for all topics. Ancestral sampling is simply:\n",
    "\n",
    "\n",
    "For each document $i$, and for each word $j=1...w_i$, do:\n",
    "\\begin{align}\n",
    "z_{i, j} &\\sim Cat(\\boldsymbol{\\theta}_i) \\\\\n",
    "w_{i, j} &\\sim Cat(\\boldsymbol{\\phi}_{z_{i,j}})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you try code it down yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Just to make things practical, we created this function for you\n",
    "## just read it or, try it, to see what it does...\n",
    "def categorical_sample(p):\n",
    "    return list(np.random.multinomial(1, p)).index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that receives document sizes J, theta (topic proportions) and phi (topics), \n",
    "## returns z (word-topic assignments ) and w (words)\n",
    "def ancestral_sampling(J, theta, phi):\n",
    "    ## Initialise\n",
    "    z = np.zeros( (I, np.max(J)), dtype=int )  #NOTICE that z and w are vectors of integers!\n",
    "    w = np.zeros( (I, np.max(J)), dtype=int )  \n",
    "\n",
    "    '''\n",
    "    TODO: write your code here\n",
    "\n",
    "    '''\n",
    "    return z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[17  7 27  5 12 17 20  6  7  5 10 17]\n",
      " [13  0  7  9 14 25 29 18 17  0  0  0]\n",
      " [22 28 20 28 25 24 22 21 23 23  0  0]]\n",
      "\n",
      "\n",
      "Document:  0\n",
      "Theta:  [0.6 0.2 0.2]\n",
      "Word 0: Topic assignment: 2->   orange  \n",
      "Word 1: Topic assignment: 1->   Kathmandu  \n",
      "Word 2: Topic assignment: 3->   neuralnetwork  \n",
      "Word 3: Topic assignment: 1->   Brussels  \n",
      "Word 4: Topic assignment: 2->   beige  \n",
      "Word 5: Topic assignment: 2->   orange  \n",
      "Word 6: Topic assignment: 3->   data  \n",
      "Word 7: Topic assignment: 1->   Beijing  \n",
      "Word 8: Topic assignment: 1->   Kathmandu  \n",
      "Word 9: Topic assignment: 1->   Brussels  \n",
      "Word 10: Topic assignment: 2->   blue  \n",
      "Word 11: Topic assignment: 2->   orange  \n",
      "\n",
      "\n",
      "Document:  1\n",
      "Theta:  [0.33333333 0.33333333 0.33333333]\n",
      "Word 0: Topic assignment: 2->   cyan  \n",
      "Word 1: Topic assignment: 1->   Copenhagen  \n",
      "Word 2: Topic assignment: 1->   Kathmandu  \n",
      "Word 3: Topic assignment: 1->   Oslo  \n",
      "Word 4: Topic assignment: 2->   black  \n",
      "Word 5: Topic assignment: 3->   dimension  \n",
      "Word 6: Topic assignment: 3->   sampling  \n",
      "Word 7: Topic assignment: 2->   white  \n",
      "Word 8: Topic assignment: 2->   orange  \n",
      "\n",
      "\n",
      "Document:  2\n",
      "Theta:  [0. 0. 1.]\n",
      "Word 0: Topic assignment: 3->   inference  \n",
      "Word 1: Topic assignment: 3->   analytics  \n",
      "Word 2: Topic assignment: 3->   data  \n",
      "Word 3: Topic assignment: 3->   analytics  \n",
      "Word 4: Topic assignment: 3->   dimension  \n",
      "Word 5: Topic assignment: 3->   observation  \n",
      "Word 6: Topic assignment: 3->   inference  \n",
      "Word 7: Topic assignment: 3->   model  \n",
      "Word 8: Topic assignment: 3->   learning  \n",
      "Word 9: Topic assignment: 3->   learning  \n"
     ]
    }
   ],
   "source": [
    "J = [12, 9, 10] ## Vector of size I denoting how many words are in each document\n",
    "z, w = ancestral_sampling(J, theta, phi)\n",
    "print(\"w:\", w)\n",
    "for i in range(I):\n",
    "    print(\"\\n\\nDocument: \", i)\n",
    "    print(\"Theta: \", theta[i])\n",
    "    for j in range(J[i]):\n",
    "        print(\"Word %d: Topic assignment: %d->   %s  \" % (j,  z[i, j]+1, dictionary[w[i, j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to take a look at the generated documents? Do they make sense, given the values for $\\phi$ and $\\theta$ given above?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bet you guessed what we want you to do next: having generated yourself the data, can you make an LDA model in STAN that recovers the original parameters $\\phi$ and $\\theta$?\n",
    "\n",
    "A dataset with only 3 documents is too little of course. So, let's generate 100 documents intead, by using the dirichlet distribution. In other words, we will generate:\n",
    "- vectors $\\theta_1 \\dots \\theta_I$\n",
    "- a new vector $J$ that contains their sizes, with $J_i \\sim Poisson(10)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=100\n",
    "theta = np.zeros( (I, K));\n",
    "alpha = 0.5*np.ones(K) ## Size K dirichlet prior\n",
    "J=[]\n",
    "\n",
    "for i in range(I):\n",
    "    theta[i] = np.random.dirichlet(alpha);\n",
    "    J.append(int(np.random.poisson(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please reuse your code to generate the dataset W. Please reuse $\\phi$ as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implement your own LDA model in STAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to make your own model in STAN! :-) Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_STAN=\"\"\"\n",
    "TODO: your code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5*np.ones(C) ## Size C dirichlet prior, for topic word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_b33a7a363a92e55a9cb937be00a710ce NOW.\n",
      "/anaconda3/lib/python3.7/site-packages/Cython/Compiler/Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /var/folders/gy/zs468fmx43v7gcy1m87k228c0000gn/T/tmplkz1zyw5/stanfit4anon_model_b33a7a363a92e55a9cb937be00a710ce_19642866712598299.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n"
     ]
    }
   ],
   "source": [
    "## Compile and collect data\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':np.max(J), 'W':W+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/var/folders/gy/zs468fmx43v7gcy1m87k228c0000gn/T/tmp_09dbxkm/output.csv`\n"
     ]
    }
   ],
   "source": [
    "## Sample with VB\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract results from STAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = pystan_utils.vb_extract_variable(fit, \"theta\", var_type=\"matrix\", dims=[I,K])\n",
    "phi_hat = pystan_utils.vb_extract_variable(fit, \"phi\", var_type=\"matrix\", dims=[K,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare estimated $\\hat{\\phi}$ with true values of $\\phi$ used to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(phi_hat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything interesting? Do the results match exactly? Remember that if we call \"topic 1\" -> \"topic 2\", and we call \"topic 2\" -> \"topic 1\", the resulting model is still the same! We just \"renamed\" the topics. This is another case of the problem of model identifiability...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
